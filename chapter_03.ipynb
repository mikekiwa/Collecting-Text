{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 3: Working with APIs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Overview of what APIs are]\n",
    "\n",
    "Most of the APIs of interest to social scientists weren't designed for our use. They are primarily for web or mobile app developers who want to include the content on their pages. So while I might use the [MapQuest](http://developer.mapquest.com) API to look at how often intra-city trips involve highways, the target audience is business owners trying to help people get to their store. Similarly, scores of researchers have used data from [Twitter APIs](https://dev.twitter.com/docs/api/1.1) to study politics, but it was developed so that you could put a custom Twitter widget on your home page. \n",
    "\n",
    "The good news is that since these services want you to use their data, the APIs are often well documented, especially for languages like Python that are popular in Silicon Valley. The bad news is that APIs don't always make available the parts of the service, like historical data, that are of most interest to researchers. The worst news is research using APIs frequently violates the providers Terms of Service, so it can be an ethical grey zone. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A first API\n",
    "\n",
    "When you sign up as a developer to use an API, you usually agree to only use the API to facilitate other people using the service (e.g. customer's finding their way to your store) and that you won't store the data. API providers usually enforce this through rate limiting, meaning you can only access the service so many times per minute or per day. For example, you can only search status updates 180 times every 15 minutes according to [Twitter guidelines](https://dev.twitter.com/docs/rate-limiting/1.1/limits). [Yelp](http://www.yelp.com/developers/documentation/faq) limits you to 10,000 calls per day. If you go over your limit, you won't be able to access the service for a bit. You will also get in trouble if you redistribute the data, so don't plan on doing that. One API developed specially for researcher is available ... [Chronicling America](http://chroniclingamerica.loc.gov/about/) is a joint project of the National Endowment for the Humanities and the Library of Congress that [More description.\n",
    "\n",
    "The website has a search function and my search for the term \"[slavery](http://chroniclingamerica.loc.gov/search/pages/results/?andtext=slavery)\" returned 404,325 results.\n",
    "\n",
    "<img src=\"images/ca_slavery_search.png\">\n",
    "\n",
    "This is great, but they make it much better for researchers by providing an API to assist with searching and downloading their archive. [Note about bulk downloads.]\n",
    "\n",
    "One of the nice things about APIs is that they are often intuitive, or at least interpretable after you see them. For example, to retrieve the first page of search results into an easily digestable format, you append ``&format=json`` to the end of the search URL ``http://chroniclingamerica.loc.gov/search/pages/results/?andtext=slavery``. In your browser, this returns a text file in the JSON format.\n",
    "\n",
    "<img src=\"images/ca_slavery_api.png\">\n",
    "\n",
    "Thankfully, the programers have made the variable names understandable. As before, the search found 404,325 results (``\"totalItems\": 404325,``). The server did not return all of these, however, just 20 of them (``\"itemsPerPage\": 20``), starting with the first result (``\"startIndex\": 1,``) and ending with the 20th (``\"endIndex\": 20,``). \n",
    "\n",
    "\n",
    "http://chroniclingamerica.loc.gov/search/pages/results/?andtext=slavery&format=json\n",
    "\n",
    "\n",
    "(http://chroniclingamerica.loc.gov/about/api/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[``requests``](http://docs.python-requests.org/en/master/) is a useful and commonly used HTTP library for python. It is not a part of the default installation, but is included with Anaconda Python Distribution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It would be possible to use the API URL and parameters directly in the requests command, but since the most likely scenario involves making repeating calls to ``requests`` as part of a loop -- the search returned less than 1% of the results -- I store the strings first. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "base_url = 'http://chroniclingamerica.loc.gov/search/pages/results/'\n",
    "parameters = '?andtext=slavery&format=json'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`requests.get()` is used for both accessing websites and APIs. The command can be modified by several arguements, but at a minimum, it requires the URL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "r = requests.get(base_url + parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`r` is a `requests` response object. Any JSON returned by the server are stored in `.json().`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "search_json = r.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "JSONs are dictionary like objects, in that they have keys (think variable names) and values. `.keys()` returns a list of the keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'totalItems', u'endIndex', u'startIndex', u'itemsPerPage', u'items']\n"
     ]
    }
   ],
   "source": [
    "print search_json.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can return the value of any key by putting the key name in brackets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "404549"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_json['totalItems']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As is often the case with results from an API, most of the keys and values are metadate about either the search or what is being returned. These are useful for knowing if the search is returning what you want, which is particularly important when you are making multiple calls to the API. \n",
    "\n",
    "The data I'm intereted in is all in `items`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<type 'list'>\n",
      "20\n"
     ]
    }
   ],
   "source": [
    "print type(search_json['items'])\n",
    "print len(search_json['items'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`items` is a list with 20 items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<type 'dict'>\n",
      "<type 'dict'>\n"
     ]
    }
   ],
   "source": [
    "print type(search_json['items'][0])\n",
    "print type(search_json['items'][19])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each of the 20 items in the list is a dictionary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'sequence', u'county', u'edition', u'frequency', u'id', u'section_label', u'city', u'date', u'title', u'end_year', u'note', u'state', u'subject', u'type', u'place_of_publication', u'start_year', u'edition_label', u'publisher', u'language', u'alt_title', u'lccn', u'country', u'ocr_eng', u'batch', u'title_normal', u'url', u'place', u'page']\n"
     ]
    }
   ],
   "source": [
    "first_item = search_json['items'][0]\n",
    "\n",
    "print first_item.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While a standard CSV file has a header row that describes the contents of each column, a JSON file has keys identifying the values found in each case. Importantly, these keys need not be the same for each item. Additionally, values don't have to be numbers of strings, but could be lists or dictionaries. For example, this JSON could have included a `newspaper` key that was a dictionary with all the metadata about the newspaper the article and issue was published, an `article` key that include the article specific information as another dictionary, and a `text` key whose value was a string with the article text.\n",
    "\n",
    "As before, we can examine the contents of a particular item, such as the publication's `title`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anti-slavery bugle. volume\n"
     ]
    }
   ],
   "source": [
    "print first_item['title']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The easiest way to view or analyze this data is to convert it to a dataset-like structure. While Python does not have a builting in dataframe type, the popular `pandas` library does. By convention, it is imported as `pd`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Make sure all columns are displayed\n",
    "pd.set_option(\"display.max_columns\",101)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pandas is prety smart about importing different JSON-type objects and converting them to dataframes with its `.DataFrame()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>alt_title</th>\n",
       "      <th>batch</th>\n",
       "      <th>city</th>\n",
       "      <th>country</th>\n",
       "      <th>county</th>\n",
       "      <th>date</th>\n",
       "      <th>edition</th>\n",
       "      <th>edition_label</th>\n",
       "      <th>end_year</th>\n",
       "      <th>frequency</th>\n",
       "      <th>id</th>\n",
       "      <th>language</th>\n",
       "      <th>lccn</th>\n",
       "      <th>note</th>\n",
       "      <th>ocr_eng</th>\n",
       "      <th>page</th>\n",
       "      <th>place</th>\n",
       "      <th>place_of_publication</th>\n",
       "      <th>publisher</th>\n",
       "      <th>section_label</th>\n",
       "      <th>sequence</th>\n",
       "      <th>start_year</th>\n",
       "      <th>state</th>\n",
       "      <th>subject</th>\n",
       "      <th>title</th>\n",
       "      <th>title_normal</th>\n",
       "      <th>type</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[]</td>\n",
       "      <td>batch_ohi_ariel_ver02</td>\n",
       "      <td>[New Lisbon, Salem]</td>\n",
       "      <td>Ohio</td>\n",
       "      <td>[Columbiana, Columbiana]</td>\n",
       "      <td>18490316</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "      <td>1861</td>\n",
       "      <td>Weekly</td>\n",
       "      <td>/lccn/sn83035487/1849-03-16/ed-1/seq-1/</td>\n",
       "      <td>[English]</td>\n",
       "      <td>sn83035487</td>\n",
       "      <td>[Archived issues are available in digital form...</td>\n",
       "      <td>LAVE\\nam\\nJlile\\nVOL. 4. NO. 30.\\nSALEM. OHIO,...</td>\n",
       "      <td></td>\n",
       "      <td>[Ohio--Columbiana--New Lisbon, Ohio--Columbian...</td>\n",
       "      <td>New-Lisbon, Ohio</td>\n",
       "      <td>Ohio American Antislavery Society</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>1845</td>\n",
       "      <td>[Ohio, Ohio]</td>\n",
       "      <td>[Antislavery movements--United States--Newspap...</td>\n",
       "      <td>Anti-slavery bugle. volume</td>\n",
       "      <td>anti-slavery bugle.</td>\n",
       "      <td>page</td>\n",
       "      <td>http://chroniclingamerica.loc.gov/lccn/sn83035...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[]</td>\n",
       "      <td>batch_iune_golf_ver01</td>\n",
       "      <td>[Chicago]</td>\n",
       "      <td>Illinois</td>\n",
       "      <td>[Cook County]</td>\n",
       "      <td>19140516</td>\n",
       "      <td>None</td>\n",
       "      <td>NOON EDITION</td>\n",
       "      <td>1917</td>\n",
       "      <td>Daily (except Sunday and holidays)</td>\n",
       "      <td>/lccn/sn83045487/1914-05-16/ed-1/seq-10/</td>\n",
       "      <td>[English]</td>\n",
       "      <td>sn83045487</td>\n",
       "      <td>[\"An adless daily newspaper.\", Archived issues...</td>\n",
       "      <td>r\\nmmmmmmmmmmmmmmmmmmmmmmmm\\n'SLAVERY RIFE IN ...</td>\n",
       "      <td></td>\n",
       "      <td>[Illinois--Cook County--Chicago]</td>\n",
       "      <td>Chicago, Ill.</td>\n",
       "      <td>N.D. Cochran</td>\n",
       "      <td></td>\n",
       "      <td>10</td>\n",
       "      <td>1911</td>\n",
       "      <td>[Illinois]</td>\n",
       "      <td>[Chicago (Ill.)--Newspapers., Illinois--Chicag...</td>\n",
       "      <td>The day book.</td>\n",
       "      <td>day book.</td>\n",
       "      <td>page</td>\n",
       "      <td>http://chroniclingamerica.loc.gov/lccn/sn83045...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[]</td>\n",
       "      <td>batch_iune_india_ver01</td>\n",
       "      <td>[Chicago]</td>\n",
       "      <td>Illinois</td>\n",
       "      <td>[Cook County]</td>\n",
       "      <td>19161109</td>\n",
       "      <td>None</td>\n",
       "      <td>EXTRA</td>\n",
       "      <td>1917</td>\n",
       "      <td>Daily (except Sunday and holidays)</td>\n",
       "      <td>/lccn/sn83045487/1916-11-09/ed-1/seq-26/</td>\n",
       "      <td>[English]</td>\n",
       "      <td>sn83045487</td>\n",
       "      <td>[\"An adless daily newspaper.\", Archived issues...</td>\n",
       "      <td>us remaining whites if we expect to\\nstay on t...</td>\n",
       "      <td></td>\n",
       "      <td>[Illinois--Cook County--Chicago]</td>\n",
       "      <td>Chicago, Ill.</td>\n",
       "      <td>N.D. Cochran</td>\n",
       "      <td></td>\n",
       "      <td>26</td>\n",
       "      <td>1911</td>\n",
       "      <td>[Illinois]</td>\n",
       "      <td>[Chicago (Ill.)--Newspapers., Illinois--Chicag...</td>\n",
       "      <td>The day book.</td>\n",
       "      <td>day book.</td>\n",
       "      <td>page</td>\n",
       "      <td>http://chroniclingamerica.loc.gov/lccn/sn83045...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[]</td>\n",
       "      <td>batch_iune_golf_ver01</td>\n",
       "      <td>[Chicago]</td>\n",
       "      <td>Illinois</td>\n",
       "      <td>[Cook County]</td>\n",
       "      <td>19150327</td>\n",
       "      <td>None</td>\n",
       "      <td>NOON EDITION</td>\n",
       "      <td>1917</td>\n",
       "      <td>Daily (except Sunday and holidays)</td>\n",
       "      <td>/lccn/sn83045487/1915-03-27/ed-1/seq-24/</td>\n",
       "      <td>[English]</td>\n",
       "      <td>sn83045487</td>\n",
       "      <td>[\"An adless daily newspaper.\", Archived issues...</td>\n",
       "      <td>THOUSANDS OF VEILED WOMEN OF TURKISH\\nHAREM ON...</td>\n",
       "      <td></td>\n",
       "      <td>[Illinois--Cook County--Chicago]</td>\n",
       "      <td>Chicago, Ill.</td>\n",
       "      <td>N.D. Cochran</td>\n",
       "      <td></td>\n",
       "      <td>24</td>\n",
       "      <td>1911</td>\n",
       "      <td>[Illinois]</td>\n",
       "      <td>[Chicago (Ill.)--Newspapers., Illinois--Chicag...</td>\n",
       "      <td>The day book.</td>\n",
       "      <td>day book.</td>\n",
       "      <td>page</td>\n",
       "      <td>http://chroniclingamerica.loc.gov/lccn/sn83045...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[]</td>\n",
       "      <td>batch_iune_foxtrot_ver01</td>\n",
       "      <td>[Chicago]</td>\n",
       "      <td>Illinois</td>\n",
       "      <td>[Cook County]</td>\n",
       "      <td>19130815</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "      <td>1917</td>\n",
       "      <td>Daily (except Sunday and holidays)</td>\n",
       "      <td>/lccn/sn83045487/1913-08-15/ed-1/seq-5/</td>\n",
       "      <td>[English]</td>\n",
       "      <td>sn83045487</td>\n",
       "      <td>[\"An adless daily newspaper.\", Archived issues...</td>\n",
       "      <td>LOLA NORRiajQlVS SiENSAT-iPN AL t EVIDENCE IN ...</td>\n",
       "      <td></td>\n",
       "      <td>[Illinois--Cook County--Chicago]</td>\n",
       "      <td>Chicago, Ill.</td>\n",
       "      <td>N.D. Cochran</td>\n",
       "      <td></td>\n",
       "      <td>5</td>\n",
       "      <td>1911</td>\n",
       "      <td>[Illinois]</td>\n",
       "      <td>[Chicago (Ill.)--Newspapers., Illinois--Chicag...</td>\n",
       "      <td>The day book.</td>\n",
       "      <td>day book.</td>\n",
       "      <td>page</td>\n",
       "      <td>http://chroniclingamerica.loc.gov/lccn/sn83045...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[]</td>\n",
       "      <td>batch_iune_foxtrot_ver01</td>\n",
       "      <td>[Chicago]</td>\n",
       "      <td>Illinois</td>\n",
       "      <td>[Cook County]</td>\n",
       "      <td>19130308</td>\n",
       "      <td>None</td>\n",
       "      <td>NOON EDITION</td>\n",
       "      <td>1917</td>\n",
       "      <td>Daily (except Sunday and holidays)</td>\n",
       "      <td>/lccn/sn83045487/1913-03-08/ed-1/seq-6/</td>\n",
       "      <td>[English]</td>\n",
       "      <td>sn83045487</td>\n",
       "      <td>[\"An adless daily newspaper.\", Archived issues...</td>\n",
       "      <td>that every possible weakness in. a\\ngirl as &amp;e...</td>\n",
       "      <td></td>\n",
       "      <td>[Illinois--Cook County--Chicago]</td>\n",
       "      <td>Chicago, Ill.</td>\n",
       "      <td>N.D. Cochran</td>\n",
       "      <td></td>\n",
       "      <td>6</td>\n",
       "      <td>1911</td>\n",
       "      <td>[Illinois]</td>\n",
       "      <td>[Chicago (Ill.)--Newspapers., Illinois--Chicag...</td>\n",
       "      <td>The day book.</td>\n",
       "      <td>day book.</td>\n",
       "      <td>page</td>\n",
       "      <td>http://chroniclingamerica.loc.gov/lccn/sn83045...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[]</td>\n",
       "      <td>batch_iune_foxtrot_ver01</td>\n",
       "      <td>[Chicago]</td>\n",
       "      <td>Illinois</td>\n",
       "      <td>[Cook County]</td>\n",
       "      <td>19130424</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "      <td>1917</td>\n",
       "      <td>Daily (except Sunday and holidays)</td>\n",
       "      <td>/lccn/sn83045487/1913-04-24/ed-1/seq-13/</td>\n",
       "      <td>[English]</td>\n",
       "      <td>sn83045487</td>\n",
       "      <td>[\"An adless daily newspaper.\", Archived issues...</td>\n",
       "      <td>mpICFED FOR WHITE -SLAVERY.\\nTop Lola Norris-a...</td>\n",
       "      <td></td>\n",
       "      <td>[Illinois--Cook County--Chicago]</td>\n",
       "      <td>Chicago, Ill.</td>\n",
       "      <td>N.D. Cochran</td>\n",
       "      <td></td>\n",
       "      <td>13</td>\n",
       "      <td>1911</td>\n",
       "      <td>[Illinois]</td>\n",
       "      <td>[Chicago (Ill.)--Newspapers., Illinois--Chicag...</td>\n",
       "      <td>The day book.</td>\n",
       "      <td>day book.</td>\n",
       "      <td>page</td>\n",
       "      <td>http://chroniclingamerica.loc.gov/lccn/sn83045...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[]</td>\n",
       "      <td>batch_dlc_elf_ver03</td>\n",
       "      <td>[Washington]</td>\n",
       "      <td>District of Columbia</td>\n",
       "      <td>[None]</td>\n",
       "      <td>18540511</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "      <td>1860</td>\n",
       "      <td>Weekly</td>\n",
       "      <td>/lccn/sn84026752/1854-05-11/ed-1/seq-1/</td>\n",
       "      <td>[English]</td>\n",
       "      <td>sn84026752</td>\n",
       "      <td>[Also issued on microfilm by University Microf...</td>\n",
       "      <td>I IiiLMI or SUBSCRimOM\\nI T. \\ .. &amp;m is publis...</td>\n",
       "      <td></td>\n",
       "      <td>[District of Columbia--Washington]</td>\n",
       "      <td>Washington [D.C.]</td>\n",
       "      <td>L.P. Noble</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>1847</td>\n",
       "      <td>[District of Columbia]</td>\n",
       "      <td>[African Americans--Washington (D.C.)--Newspap...</td>\n",
       "      <td>The national era.</td>\n",
       "      <td>national era.</td>\n",
       "      <td>page</td>\n",
       "      <td>http://chroniclingamerica.loc.gov/lccn/sn84026...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[]</td>\n",
       "      <td>batch_iune_foxtrot_ver01</td>\n",
       "      <td>[Chicago]</td>\n",
       "      <td>Illinois</td>\n",
       "      <td>[Cook County]</td>\n",
       "      <td>19130225</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "      <td>1917</td>\n",
       "      <td>Daily (except Sunday and holidays)</td>\n",
       "      <td>/lccn/sn83045487/1913-02-25/ed-1/seq-30/</td>\n",
       "      <td>[English]</td>\n",
       "      <td>sn83045487</td>\n",
       "      <td>[\"An adless daily newspaper.\", Archived issues...</td>\n",
       "      <td>we are doing what the American\\nmen did Avay b...</td>\n",
       "      <td></td>\n",
       "      <td>[Illinois--Cook County--Chicago]</td>\n",
       "      <td>Chicago, Ill.</td>\n",
       "      <td>N.D. Cochran</td>\n",
       "      <td></td>\n",
       "      <td>30</td>\n",
       "      <td>1911</td>\n",
       "      <td>[Illinois]</td>\n",
       "      <td>[Chicago (Ill.)--Newspapers., Illinois--Chicag...</td>\n",
       "      <td>The day book.</td>\n",
       "      <td>day book.</td>\n",
       "      <td>page</td>\n",
       "      <td>http://chroniclingamerica.loc.gov/lccn/sn83045...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[]</td>\n",
       "      <td>batch_dlc_elf_ver03</td>\n",
       "      <td>[Washington]</td>\n",
       "      <td>District of Columbia</td>\n",
       "      <td>[None]</td>\n",
       "      <td>18540511</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "      <td>1860</td>\n",
       "      <td>Weekly</td>\n",
       "      <td>/lccn/sn84026752/1854-05-11/ed-1/seq-4/</td>\n",
       "      <td>[English]</td>\n",
       "      <td>sn84026752</td>\n",
       "      <td>[Also issued on microfilm by University Microf...</td>\n",
       "      <td>f\\nI 76\\n[COXTIHCED PBOM KIMT PAGE.]\\nour fath...</td>\n",
       "      <td>76</td>\n",
       "      <td>[District of Columbia--Washington]</td>\n",
       "      <td>Washington [D.C.]</td>\n",
       "      <td>L.P. Noble</td>\n",
       "      <td></td>\n",
       "      <td>4</td>\n",
       "      <td>1847</td>\n",
       "      <td>[District of Columbia]</td>\n",
       "      <td>[African Americans--Washington (D.C.)--Newspap...</td>\n",
       "      <td>The national era.</td>\n",
       "      <td>national era.</td>\n",
       "      <td>page</td>\n",
       "      <td>http://chroniclingamerica.loc.gov/lccn/sn84026...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  alt_title                     batch                 city  \\\n",
       "0        []     batch_ohi_ariel_ver02  [New Lisbon, Salem]   \n",
       "1        []     batch_iune_golf_ver01            [Chicago]   \n",
       "2        []    batch_iune_india_ver01            [Chicago]   \n",
       "3        []     batch_iune_golf_ver01            [Chicago]   \n",
       "4        []  batch_iune_foxtrot_ver01            [Chicago]   \n",
       "5        []  batch_iune_foxtrot_ver01            [Chicago]   \n",
       "6        []  batch_iune_foxtrot_ver01            [Chicago]   \n",
       "7        []       batch_dlc_elf_ver03         [Washington]   \n",
       "8        []  batch_iune_foxtrot_ver01            [Chicago]   \n",
       "9        []       batch_dlc_elf_ver03         [Washington]   \n",
       "\n",
       "                country                    county      date edition  \\\n",
       "0                  Ohio  [Columbiana, Columbiana]  18490316    None   \n",
       "1              Illinois             [Cook County]  19140516    None   \n",
       "2              Illinois             [Cook County]  19161109    None   \n",
       "3              Illinois             [Cook County]  19150327    None   \n",
       "4              Illinois             [Cook County]  19130815    None   \n",
       "5              Illinois             [Cook County]  19130308    None   \n",
       "6              Illinois             [Cook County]  19130424    None   \n",
       "7  District of Columbia                    [None]  18540511    None   \n",
       "8              Illinois             [Cook County]  19130225    None   \n",
       "9  District of Columbia                    [None]  18540511    None   \n",
       "\n",
       "  edition_label  end_year                           frequency  \\\n",
       "0                    1861                              Weekly   \n",
       "1  NOON EDITION      1917  Daily (except Sunday and holidays)   \n",
       "2         EXTRA      1917  Daily (except Sunday and holidays)   \n",
       "3  NOON EDITION      1917  Daily (except Sunday and holidays)   \n",
       "4                    1917  Daily (except Sunday and holidays)   \n",
       "5  NOON EDITION      1917  Daily (except Sunday and holidays)   \n",
       "6                    1917  Daily (except Sunday and holidays)   \n",
       "7                    1860                              Weekly   \n",
       "8                    1917  Daily (except Sunday and holidays)   \n",
       "9                    1860                              Weekly   \n",
       "\n",
       "                                         id   language        lccn  \\\n",
       "0   /lccn/sn83035487/1849-03-16/ed-1/seq-1/  [English]  sn83035487   \n",
       "1  /lccn/sn83045487/1914-05-16/ed-1/seq-10/  [English]  sn83045487   \n",
       "2  /lccn/sn83045487/1916-11-09/ed-1/seq-26/  [English]  sn83045487   \n",
       "3  /lccn/sn83045487/1915-03-27/ed-1/seq-24/  [English]  sn83045487   \n",
       "4   /lccn/sn83045487/1913-08-15/ed-1/seq-5/  [English]  sn83045487   \n",
       "5   /lccn/sn83045487/1913-03-08/ed-1/seq-6/  [English]  sn83045487   \n",
       "6  /lccn/sn83045487/1913-04-24/ed-1/seq-13/  [English]  sn83045487   \n",
       "7   /lccn/sn84026752/1854-05-11/ed-1/seq-1/  [English]  sn84026752   \n",
       "8  /lccn/sn83045487/1913-02-25/ed-1/seq-30/  [English]  sn83045487   \n",
       "9   /lccn/sn84026752/1854-05-11/ed-1/seq-4/  [English]  sn84026752   \n",
       "\n",
       "                                                note  \\\n",
       "0  [Archived issues are available in digital form...   \n",
       "1  [\"An adless daily newspaper.\", Archived issues...   \n",
       "2  [\"An adless daily newspaper.\", Archived issues...   \n",
       "3  [\"An adless daily newspaper.\", Archived issues...   \n",
       "4  [\"An adless daily newspaper.\", Archived issues...   \n",
       "5  [\"An adless daily newspaper.\", Archived issues...   \n",
       "6  [\"An adless daily newspaper.\", Archived issues...   \n",
       "7  [Also issued on microfilm by University Microf...   \n",
       "8  [\"An adless daily newspaper.\", Archived issues...   \n",
       "9  [Also issued on microfilm by University Microf...   \n",
       "\n",
       "                                             ocr_eng page  \\\n",
       "0  LAVE\\nam\\nJlile\\nVOL. 4. NO. 30.\\nSALEM. OHIO,...        \n",
       "1  r\\nmmmmmmmmmmmmmmmmmmmmmmmm\\n'SLAVERY RIFE IN ...        \n",
       "2  us remaining whites if we expect to\\nstay on t...        \n",
       "3  THOUSANDS OF VEILED WOMEN OF TURKISH\\nHAREM ON...        \n",
       "4  LOLA NORRiajQlVS SiENSAT-iPN AL t EVIDENCE IN ...        \n",
       "5  that every possible weakness in. a\\ngirl as &e...        \n",
       "6  mpICFED FOR WHITE -SLAVERY.\\nTop Lola Norris-a...        \n",
       "7  I IiiLMI or SUBSCRimOM\\nI T. \\ .. &m is publis...        \n",
       "8  we are doing what the American\\nmen did Avay b...        \n",
       "9  f\\nI 76\\n[COXTIHCED PBOM KIMT PAGE.]\\nour fath...   76   \n",
       "\n",
       "                                               place place_of_publication  \\\n",
       "0  [Ohio--Columbiana--New Lisbon, Ohio--Columbian...     New-Lisbon, Ohio   \n",
       "1                   [Illinois--Cook County--Chicago]        Chicago, Ill.   \n",
       "2                   [Illinois--Cook County--Chicago]        Chicago, Ill.   \n",
       "3                   [Illinois--Cook County--Chicago]        Chicago, Ill.   \n",
       "4                   [Illinois--Cook County--Chicago]        Chicago, Ill.   \n",
       "5                   [Illinois--Cook County--Chicago]        Chicago, Ill.   \n",
       "6                   [Illinois--Cook County--Chicago]        Chicago, Ill.   \n",
       "7                 [District of Columbia--Washington]    Washington [D.C.]   \n",
       "8                   [Illinois--Cook County--Chicago]        Chicago, Ill.   \n",
       "9                 [District of Columbia--Washington]    Washington [D.C.]   \n",
       "\n",
       "                           publisher section_label  sequence  start_year  \\\n",
       "0  Ohio American Antislavery Society                       1        1845   \n",
       "1                       N.D. Cochran                      10        1911   \n",
       "2                       N.D. Cochran                      26        1911   \n",
       "3                       N.D. Cochran                      24        1911   \n",
       "4                       N.D. Cochran                       5        1911   \n",
       "5                       N.D. Cochran                       6        1911   \n",
       "6                       N.D. Cochran                      13        1911   \n",
       "7                         L.P. Noble                       1        1847   \n",
       "8                       N.D. Cochran                      30        1911   \n",
       "9                         L.P. Noble                       4        1847   \n",
       "\n",
       "                    state                                            subject  \\\n",
       "0            [Ohio, Ohio]  [Antislavery movements--United States--Newspap...   \n",
       "1              [Illinois]  [Chicago (Ill.)--Newspapers., Illinois--Chicag...   \n",
       "2              [Illinois]  [Chicago (Ill.)--Newspapers., Illinois--Chicag...   \n",
       "3              [Illinois]  [Chicago (Ill.)--Newspapers., Illinois--Chicag...   \n",
       "4              [Illinois]  [Chicago (Ill.)--Newspapers., Illinois--Chicag...   \n",
       "5              [Illinois]  [Chicago (Ill.)--Newspapers., Illinois--Chicag...   \n",
       "6              [Illinois]  [Chicago (Ill.)--Newspapers., Illinois--Chicag...   \n",
       "7  [District of Columbia]  [African Americans--Washington (D.C.)--Newspap...   \n",
       "8              [Illinois]  [Chicago (Ill.)--Newspapers., Illinois--Chicag...   \n",
       "9  [District of Columbia]  [African Americans--Washington (D.C.)--Newspap...   \n",
       "\n",
       "                        title         title_normal  type  \\\n",
       "0  Anti-slavery bugle. volume  anti-slavery bugle.  page   \n",
       "1               The day book.            day book.  page   \n",
       "2               The day book.            day book.  page   \n",
       "3               The day book.            day book.  page   \n",
       "4               The day book.            day book.  page   \n",
       "5               The day book.            day book.  page   \n",
       "6               The day book.            day book.  page   \n",
       "7           The national era.        national era.  page   \n",
       "8               The day book.            day book.  page   \n",
       "9           The national era.        national era.  page   \n",
       "\n",
       "                                                 url  \n",
       "0  http://chroniclingamerica.loc.gov/lccn/sn83035...  \n",
       "1  http://chroniclingamerica.loc.gov/lccn/sn83045...  \n",
       "2  http://chroniclingamerica.loc.gov/lccn/sn83045...  \n",
       "3  http://chroniclingamerica.loc.gov/lccn/sn83045...  \n",
       "4  http://chroniclingamerica.loc.gov/lccn/sn83045...  \n",
       "5  http://chroniclingamerica.loc.gov/lccn/sn83045...  \n",
       "6  http://chroniclingamerica.loc.gov/lccn/sn83045...  \n",
       "7  http://chroniclingamerica.loc.gov/lccn/sn84026...  \n",
       "8  http://chroniclingamerica.loc.gov/lccn/sn83045...  \n",
       "9  http://chroniclingamerica.loc.gov/lccn/sn84026...  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(search_json['items'])\n",
    "\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that I converted `search_json['items']` to  dataframe and not the entire JSON file. This is because I wanted each row to be an article. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If this dataframe contained all the items that you were looking for, it would be easy to save this to a csv file for storage and later analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.to_csv('lynching_articles.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is only a small subset of the articles on lynching that are available, however. The API returns results in batches of 20 and this is only the first page of results. As is often the case, I'll need to make multiple calls to the API to retrieve all the data of interest. The easiest way to do that is to define a small function for getting the article information and put that in a loop. While it isn't a requirement that you create a function for making the API call, it will make your code easier to read and debug.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the API guidelines, there is an additional paramater `page` that tells the API which subset of results we want. This name varies by API but their is usually some mechanism for retrieiving results beyond the initial JSON.\n",
    "\n",
    "Before creating the loop and making multiple calls to the API, I want to make sure that the API is working the way I think it is. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41\n",
      "60\n"
     ]
    }
   ],
   "source": [
    "base_url = 'http://chroniclingamerica.loc.gov/search/pages/results/'\n",
    "parameters = '?andtext=slavery&format=json&page=3'\n",
    "r = requests.get(base_url + parameters)\n",
    "results =  r.json()\n",
    "\n",
    "print results['startIndex']\n",
    "print results['endIndex']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A call to random selected page 3 returns results 41 through 60, which is what I expected since each page has 20 items.\n",
    "\n",
    "The parameters are getting pretty ugly, so fortunately `requests` accepts a dictionary where the keys are the parameter names as defined by the API and the values are the search paramaters you are looking for. So the same request can be rewritten:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41\n",
      "60\n"
     ]
    }
   ],
   "source": [
    "base_url = 'http://chroniclingamerica.loc.gov/search/pages/results/'\n",
    "parameters = {'andtext': 'lynching',\n",
    "              'page' : 3,\n",
    "              'format'  : 'json'}\n",
    "r = requests.get(base_url, params=parameters)\n",
    "\n",
    "results =  r.json()\n",
    "\n",
    "print results['startIndex']\n",
    "print results['endIndex']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This can be rewritten as function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_articles():\n",
    "    '''\n",
    "    Make calls to the Chronicling America API.\n",
    "    '''\n",
    "    base_url = 'http://chroniclingamerica.loc.gov/search/pages/results/'\n",
    "    parameters = {'andtext': 'lynching',\n",
    "                  'page' : 3,\n",
    "              'format'  : 'json'}\n",
    "    r = requests.get(base_url, params=parameters)\n",
    "    results =  r.json()\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41\n",
      "60\n"
     ]
    }
   ],
   "source": [
    "results = get_articles()\n",
    "\n",
    "print results['startIndex']\n",
    "print results['endIndex']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The advantage of writing a function, however, would be that you can pass along your own parameters, such as the search term and page number, which would make this much more useful. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_articles(search_term, page_number):\n",
    "    '''\n",
    "    Make calls to the Chronicling America API.\n",
    "    '''\n",
    "    base_url = 'http://chroniclingamerica.loc.gov/search/pages/results/'\n",
    "    parameters = {'andtext': search_term,\n",
    "                  'page' : page_number,\n",
    "              'format'  : 'json'}\n",
    "    r = requests.get(base_url, params=parameters)\n",
    "    results =  r.json()\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41\n",
      "60\n"
     ]
    }
   ],
   "source": [
    "results = get_articles('lynching', 3)\n",
    "\n",
    "print results['startIndex']\n",
    "print results['endIndex']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the first 60 results could downloaded in a just a few lines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 20\n",
      "21 40\n",
      "41 60\n"
     ]
    }
   ],
   "source": [
    "for page_number in range(1,4): # range stops before it gets to the last number\n",
    "    results = get_articles('lynching', page_number)\n",
    "    print results['startIndex'], results['endIndex']\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Everything appears to be working, but unfortunately I only have the last page of results still. Each call to the API was redefining `results` variable. In this case, I set up an empty dataframe to store the results and will append the items from each page of results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>alt_title</th>\n",
       "      <th>batch</th>\n",
       "      <th>city</th>\n",
       "      <th>country</th>\n",
       "      <th>county</th>\n",
       "      <th>date</th>\n",
       "      <th>edition</th>\n",
       "      <th>edition_label</th>\n",
       "      <th>end_year</th>\n",
       "      <th>frequency</th>\n",
       "      <th>id</th>\n",
       "      <th>language</th>\n",
       "      <th>lccn</th>\n",
       "      <th>note</th>\n",
       "      <th>ocr_eng</th>\n",
       "      <th>page</th>\n",
       "      <th>place</th>\n",
       "      <th>place_of_publication</th>\n",
       "      <th>publisher</th>\n",
       "      <th>section_label</th>\n",
       "      <th>sequence</th>\n",
       "      <th>start_year</th>\n",
       "      <th>state</th>\n",
       "      <th>subject</th>\n",
       "      <th>title</th>\n",
       "      <th>title_normal</th>\n",
       "      <th>type</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[]</td>\n",
       "      <td>batch_mimtptc_jackson_ver01</td>\n",
       "      <td>[Dearborn]</td>\n",
       "      <td>Michigan</td>\n",
       "      <td>[Wayne]</td>\n",
       "      <td>19211022</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "      <td>1927</td>\n",
       "      <td>Weekly</td>\n",
       "      <td>/lccn/2013218776/1921-10-22/ed-1/seq-1/</td>\n",
       "      <td>[English]</td>\n",
       "      <td>2013218776</td>\n",
       "      <td>[\"The Ford international weekly\" appears with ...</td>\n",
       "      <td>\"Mis-Picturing Us Abroad\" Introduces the Serie...</td>\n",
       "      <td></td>\n",
       "      <td>[Michigan--Wayne--Dearborn]</td>\n",
       "      <td>Dearborn, Mich.</td>\n",
       "      <td>Suburban Pub. Co.</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>1901</td>\n",
       "      <td>[Michigan]</td>\n",
       "      <td>[Dearborn (Mich.)--Newspapers., Michigan--Dear...</td>\n",
       "      <td>Dearborn independent.</td>\n",
       "      <td>dearborn independent.</td>\n",
       "      <td>page</td>\n",
       "      <td>http://chroniclingamerica.loc.gov/lccn/2013218...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[]</td>\n",
       "      <td>batch_iune_hotel_ver01</td>\n",
       "      <td>[Chicago]</td>\n",
       "      <td>Illinois</td>\n",
       "      <td>[Cook County]</td>\n",
       "      <td>19150818</td>\n",
       "      <td>None</td>\n",
       "      <td>LAST EDITION</td>\n",
       "      <td>1917</td>\n",
       "      <td>Daily (except Sunday and holidays)</td>\n",
       "      <td>/lccn/sn83045487/1915-08-18/ed-1/seq-4/</td>\n",
       "      <td>[English]</td>\n",
       "      <td>sn83045487</td>\n",
       "      <td>[\"An adless daily newspaper.\", Archived issues...</td>\n",
       "      <td>25 patriots who took into their own\\nhands a l...</td>\n",
       "      <td></td>\n",
       "      <td>[Illinois--Cook County--Chicago]</td>\n",
       "      <td>Chicago, Ill.</td>\n",
       "      <td>N.D. Cochran</td>\n",
       "      <td></td>\n",
       "      <td>4</td>\n",
       "      <td>1911</td>\n",
       "      <td>[Illinois]</td>\n",
       "      <td>[Chicago (Ill.)--Newspapers., Illinois--Chicag...</td>\n",
       "      <td>The day book.</td>\n",
       "      <td>day book.</td>\n",
       "      <td>page</td>\n",
       "      <td>http://chroniclingamerica.loc.gov/lccn/sn83045...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[Star, Sunday star]</td>\n",
       "      <td>batch_dlc_dalek_ver01</td>\n",
       "      <td>[Washington]</td>\n",
       "      <td>District of Columbia</td>\n",
       "      <td>[None]</td>\n",
       "      <td>19221123</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "      <td>1972</td>\n",
       "      <td>Daily</td>\n",
       "      <td>/lccn/sn83045462/1922-11-23/ed-1/seq-34/</td>\n",
       "      <td>[English]</td>\n",
       "      <td>sn83045462</td>\n",
       "      <td>[\"From April 25 through May 24, 1861 one sheet...</td>\n",
       "      <td>T\\nTU^\\nnit;\\n- V E\\nxl\\nII\\nb&lt;\\nin rour\\n3436...</td>\n",
       "      <td>34</td>\n",
       "      <td>[District of Columbia--Washington]</td>\n",
       "      <td>Washington, D.C.</td>\n",
       "      <td>W.D. Wallach &amp; Hope</td>\n",
       "      <td></td>\n",
       "      <td>34</td>\n",
       "      <td>1854</td>\n",
       "      <td>[District of Columbia]</td>\n",
       "      <td>[Washington (D.C.)--fast--(OCoLC)fst01204505, ...</td>\n",
       "      <td>Evening star.</td>\n",
       "      <td>evening star.</td>\n",
       "      <td>page</td>\n",
       "      <td>http://chroniclingamerica.loc.gov/lccn/sn83045...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[]</td>\n",
       "      <td>batch_wa_elm_ver01</td>\n",
       "      <td>[Seattle]</td>\n",
       "      <td>Washington</td>\n",
       "      <td>[King]</td>\n",
       "      <td>19180608</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "      <td>1921</td>\n",
       "      <td>Weekly</td>\n",
       "      <td>/lccn/sn87093353/1918-06-08/ed-1/seq-4/</td>\n",
       "      <td>[English]</td>\n",
       "      <td>sn87093353</td>\n",
       "      <td>[\"A publication of general information, but in...</td>\n",
       "      <td>DAMNABLE WHITE WHELPS\\nSome of the white subsc...</td>\n",
       "      <td></td>\n",
       "      <td>[Washington--King--Seattle]</td>\n",
       "      <td>Seattle, Wash.</td>\n",
       "      <td>H.R. Cayton</td>\n",
       "      <td></td>\n",
       "      <td>4</td>\n",
       "      <td>1916</td>\n",
       "      <td>[Washington]</td>\n",
       "      <td>[African Americans--Washington (State)--Seattl...</td>\n",
       "      <td>Cayton's weekly.</td>\n",
       "      <td>cayton's weekly.</td>\n",
       "      <td>page</td>\n",
       "      <td>http://chroniclingamerica.loc.gov/lccn/sn87093...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[]</td>\n",
       "      <td>batch_iune_delta_ver01</td>\n",
       "      <td>[Chicago]</td>\n",
       "      <td>Illinois</td>\n",
       "      <td>[Cook County]</td>\n",
       "      <td>19130507</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "      <td>1917</td>\n",
       "      <td>Daily (except Sunday and holidays)</td>\n",
       "      <td>/lccn/sn83045487/1913-05-07/ed-1/seq-12/</td>\n",
       "      <td>[English]</td>\n",
       "      <td>sn83045487</td>\n",
       "      <td>[\"An adless daily newspaper.\", Archived issues...</td>\n",
       "      <td>TEN COMPANIES OF MILITIA PROTECT ..THE\\nALLEGE...</td>\n",
       "      <td></td>\n",
       "      <td>[Illinois--Cook County--Chicago]</td>\n",
       "      <td>Chicago, Ill.</td>\n",
       "      <td>N.D. Cochran</td>\n",
       "      <td></td>\n",
       "      <td>12</td>\n",
       "      <td>1911</td>\n",
       "      <td>[Illinois]</td>\n",
       "      <td>[Chicago (Ill.)--Newspapers., Illinois--Chicag...</td>\n",
       "      <td>The day book.</td>\n",
       "      <td>day book.</td>\n",
       "      <td>page</td>\n",
       "      <td>http://chroniclingamerica.loc.gov/lccn/sn83045...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             alt_title                        batch          city  \\\n",
       "0                   []  batch_mimtptc_jackson_ver01    [Dearborn]   \n",
       "1                   []       batch_iune_hotel_ver01     [Chicago]   \n",
       "2  [Star, Sunday star]        batch_dlc_dalek_ver01  [Washington]   \n",
       "3                   []           batch_wa_elm_ver01     [Seattle]   \n",
       "4                   []       batch_iune_delta_ver01     [Chicago]   \n",
       "\n",
       "                country         county      date edition edition_label  \\\n",
       "0              Michigan        [Wayne]  19211022    None                 \n",
       "1              Illinois  [Cook County]  19150818    None  LAST EDITION   \n",
       "2  District of Columbia         [None]  19221123    None                 \n",
       "3            Washington         [King]  19180608    None                 \n",
       "4              Illinois  [Cook County]  19130507    None                 \n",
       "\n",
       "   end_year                           frequency  \\\n",
       "0      1927                              Weekly   \n",
       "1      1917  Daily (except Sunday and holidays)   \n",
       "2      1972                               Daily   \n",
       "3      1921                              Weekly   \n",
       "4      1917  Daily (except Sunday and holidays)   \n",
       "\n",
       "                                         id   language        lccn  \\\n",
       "0   /lccn/2013218776/1921-10-22/ed-1/seq-1/  [English]  2013218776   \n",
       "1   /lccn/sn83045487/1915-08-18/ed-1/seq-4/  [English]  sn83045487   \n",
       "2  /lccn/sn83045462/1922-11-23/ed-1/seq-34/  [English]  sn83045462   \n",
       "3   /lccn/sn87093353/1918-06-08/ed-1/seq-4/  [English]  sn87093353   \n",
       "4  /lccn/sn83045487/1913-05-07/ed-1/seq-12/  [English]  sn83045487   \n",
       "\n",
       "                                                note  \\\n",
       "0  [\"The Ford international weekly\" appears with ...   \n",
       "1  [\"An adless daily newspaper.\", Archived issues...   \n",
       "2  [\"From April 25 through May 24, 1861 one sheet...   \n",
       "3  [\"A publication of general information, but in...   \n",
       "4  [\"An adless daily newspaper.\", Archived issues...   \n",
       "\n",
       "                                             ocr_eng page  \\\n",
       "0  \"Mis-Picturing Us Abroad\" Introduces the Serie...        \n",
       "1  25 patriots who took into their own\\nhands a l...        \n",
       "2  T\\nTU^\\nnit;\\n- V E\\nxl\\nII\\nb<\\nin rour\\n3436...   34   \n",
       "3  DAMNABLE WHITE WHELPS\\nSome of the white subsc...        \n",
       "4  TEN COMPANIES OF MILITIA PROTECT ..THE\\nALLEGE...        \n",
       "\n",
       "                                place place_of_publication  \\\n",
       "0         [Michigan--Wayne--Dearborn]      Dearborn, Mich.   \n",
       "1    [Illinois--Cook County--Chicago]        Chicago, Ill.   \n",
       "2  [District of Columbia--Washington]     Washington, D.C.   \n",
       "3         [Washington--King--Seattle]       Seattle, Wash.   \n",
       "4    [Illinois--Cook County--Chicago]        Chicago, Ill.   \n",
       "\n",
       "             publisher section_label  sequence  start_year  \\\n",
       "0    Suburban Pub. Co.                       1        1901   \n",
       "1         N.D. Cochran                       4        1911   \n",
       "2  W.D. Wallach & Hope                      34        1854   \n",
       "3          H.R. Cayton                       4        1916   \n",
       "4         N.D. Cochran                      12        1911   \n",
       "\n",
       "                    state                                            subject  \\\n",
       "0              [Michigan]  [Dearborn (Mich.)--Newspapers., Michigan--Dear...   \n",
       "1              [Illinois]  [Chicago (Ill.)--Newspapers., Illinois--Chicag...   \n",
       "2  [District of Columbia]  [Washington (D.C.)--fast--(OCoLC)fst01204505, ...   \n",
       "3            [Washington]  [African Americans--Washington (State)--Seattl...   \n",
       "4              [Illinois]  [Chicago (Ill.)--Newspapers., Illinois--Chicag...   \n",
       "\n",
       "                   title           title_normal  type  \\\n",
       "0  Dearborn independent.  dearborn independent.  page   \n",
       "1          The day book.              day book.  page   \n",
       "2          Evening star.          evening star.  page   \n",
       "3       Cayton's weekly.       cayton's weekly.  page   \n",
       "4          The day book.              day book.  page   \n",
       "\n",
       "                                                 url  \n",
       "0  http://chroniclingamerica.loc.gov/lccn/2013218...  \n",
       "1  http://chroniclingamerica.loc.gov/lccn/sn83045...  \n",
       "2  http://chroniclingamerica.loc.gov/lccn/sn83045...  \n",
       "3  http://chroniclingamerica.loc.gov/lccn/sn87093...  \n",
       "4  http://chroniclingamerica.loc.gov/lccn/sn83045...  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame()\n",
    "\n",
    "for page_number in range(1,4):\n",
    "    results = get_articles('lynching', page_number)\n",
    "    new_df = pd.DataFrame(results['items'])\n",
    "    df = df.append(new_df , ignore_index=True)\n",
    "    \n",
    "print len(df)\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a large download, you would still want to tweak this a bit by pausing between each API call and making it robust to internet or API errors, but this is a solid framework for collecting data from an API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A second API\n",
    "\n",
    "While the Chronicling America API allows annonymous usage, most APIs require you to register in advance. This usually involves going to their website, signing up for the service, and then going through a second signup for developers.  \n",
    "When you sign up  to use an API, you usually agree to only use the API to facilitate other people using the service (e.g. customer's finding their way to your store) and that you won't store the data. API providers usually enforce this through rate limiting, meaning you can only access the service so many times per minute or per day. For example, you can only search status updates 180 times every 15 minutes according to [Twitter guidelines](https://dev.twitter.com/docs/rate-limiting/1.1/limits). [Yelp](http://www.yelp.com/developers/documentation/faq) limits you to 10,000 calls per day. If you go over your limit, you won't be able to access the service for a bit. You will also get in trouble if you redistribute the data, so don't plan on doing that. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two of the major reasons that web services require API authentication is so that they know who you are and so they can make sure that you don't go over their rate limits. Since you shouldn't be giving your password to random people on the internet, API authentication works a little bit differently. Like many other places, in order to use the Yelp API you have to sign up as [developer](http://www.yelp.com/developers). After telling them a little bit about what you plan to do--feel free to be honest; they aren't going to deny you access if you put \"research on food cultures\" as the purpose--you will get a Consumer Key, Consumer Secret, Token, and Token Secret. Copy and paste them somewhere special. \n",
    "\n",
    "Using the Yelp API goes something like this. First, you tell Yelp who you are and what you want. Assuming you are authorized to have this information, they respond with a URL where you can retrieve the data. The coding for this in practice is a little bit complicated, so there are often single use tools for accessing APIs, like [Tweepy](http://tweepy.github.io) for Twitter. \n",
    "\n",
    "Yelp uses the OAuth protocol for authentication. There are several python libraries for handling this, but you will likely need to install one (via `conda` or `pip`) yourself first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import oauth2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "There's no module to install for the Yelp API, but Yelp does provide some [sample Python code](https://github.com/Yelp/yelp-api/tree/master/v2/python). I've slightly modified the code below to show a sample search for restaurants near Chapel Hill, NC, sorted by distance. You can find more options in the search [documentation](http://www.yelp.com/developers/documentation/v2/search_api). The API's search options include things like location and type of business, and allows you to sort either by distance or popularity.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://api.yelp.com/v2/search?sort=1&oauth_body_hash=2jmj7l5rSw0yVb%2FvlWAYkK%2FYBwk%3D&oauth_nonce=35980538&oauth_timestamp=1475698040&oauth_consumer_key=qDBPo9c_szHVrZwxzo-zDw&oauth_signature_method=HMAC-SHA1&category_filter=restaurants&oauth_token=jeRrhRey_k-emvC_VFLGrlVHrkR4P3UF&location=Chapel+Hill%2C+NC&oauth_signature=FVpomqjb96J3rG2Eb5bLmzaCSrs%3D\n"
     ]
    }
   ],
   "source": [
    "consumer_key    = 'qDBPo9c_szHVrZwxzo-zDw'\n",
    "consumer_secret = '4we8Jz9rq5J3j15Z5yCUqmgDJjM'\n",
    "token           = 'jeRrhRey_k-emvC_VFLGrlVHrkR4P3UF'\n",
    "token_secret    = 'n-7xHNCxxedmAMYZPQtnh1hd7lI'\n",
    "\n",
    "consumer = oauth2.Consumer(consumer_key, consumer_secret)\n",
    "\n",
    "category_filter = 'restaurants'\n",
    "location = 'Chapel Hill, NC'\n",
    "options =  'category_filter=%s&location=%s&sort=1' % (category_filter, location)\n",
    "url = 'http://api.yelp.com/v2/search?' + options\n",
    "\n",
    "oauth_request = oauth2.Request('GET', url, {})\n",
    "oauth_request.update({'oauth_nonce'      : oauth2.generate_nonce(),\n",
    "                      'oauth_timestamp'  : oauth2.generate_timestamp(),\n",
    "                      'oauth_token'       : token,\n",
    "                      'oauth_consumer_key': consumer_key})\n",
    "\n",
    "token = oauth2.Token(token, token_secret)\n",
    "oauth_request.sign_request(oauth2.SignatureMethod_HMAC_SHA1(), consumer, token)\n",
    "signed_url = oauth_request.to_url()\n",
    "\n",
    "print signed_url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The URL returned expires after a couple of seconds, so don't expect for the above link to work. The results are provided in the JSON file format, so I'm going to use the already imported `requests` module to download them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'region', u'total', u'businesses']\n"
     ]
    }
   ],
   "source": [
    "resp = requests.get(url=signed_url)\n",
    "chapel_hill_restaurants = resp.json()\n",
    "\n",
    "print chapel_hill_restaurants.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with the Chronacling America API, the top level of the JSON contains some metadata about the search with all the specific items in one field. In this case, `businesses`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{u'categories': [[u'Burgers', u'burgers'], [u'Hot Dogs', u'hotdog']],\n",
       " u'display_phone': u'+1-919-240-4746',\n",
       " u'id': u'buns-chapel-hill',\n",
       " u'image_url': u'https://s3-media1.fl.yelpcdn.com/bphoto/5VCBOVZXf0y5TmIVTTuD1w/ms.jpg',\n",
       " u'is_claimed': True,\n",
       " u'is_closed': False,\n",
       " u'location': {u'address': [u'107 N Columbia St'],\n",
       "  u'city': u'Chapel Hill',\n",
       "  u'coordinate': {u'latitude': 35.913498068127,\n",
       "   u'longitude': -79.056107631617},\n",
       "  u'country_code': u'US',\n",
       "  u'display_address': [u'107 N Columbia St', u'Chapel Hill, NC 27514'],\n",
       "  u'geo_accuracy': 9.5,\n",
       "  u'postal_code': u'27514',\n",
       "  u'state_code': u'NC'},\n",
       " u'mobile_url': u'https://m.yelp.com/biz/buns-chapel-hill?adjust_creative=qDBPo9c_szHVrZwxzo-zDw&utm_campaign=yelp_api&utm_medium=api_v2_search&utm_source=qDBPo9c_szHVrZwxzo-zDw',\n",
       " u'name': u'Buns',\n",
       " u'phone': u'9192404746',\n",
       " u'rating': 4.0,\n",
       " u'rating_img_url': u'https://s3-media4.fl.yelpcdn.com/assets/2/www/img/c2f3dd9799a5/ico/stars/v1/stars_4.png',\n",
       " u'rating_img_url_large': u'https://s3-media2.fl.yelpcdn.com/assets/2/www/img/ccf2b76faa2c/ico/stars/v1/stars_large_4.png',\n",
       " u'rating_img_url_small': u'https://s3-media4.fl.yelpcdn.com/assets/2/www/img/f62a5be2f902/ico/stars/v1/stars_small_4.png',\n",
       " u'review_count': 280,\n",
       " u'snippet_image_url': u'https://s3-media1.fl.yelpcdn.com/photo/HHBYKIwnQh8afZUBydhpIQ/ms.jpg',\n",
       " u'snippet_text': u\"I can't get enough of their Goat Cheese burger. Add caramelozed onions and tomato and you have THE most mouth watering burger. Seriously. \\n\\nOn Tuesdays you...\",\n",
       " u'url': u'https://www.yelp.com/biz/buns-chapel-hill?adjust_creative=qDBPo9c_szHVrZwxzo-zDw&utm_campaign=yelp_api&utm_medium=api_v2_search&utm_source=qDBPo9c_szHVrZwxzo-zDw'}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chapel_hill_restaurants['businesses'][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspecting the returned results for one restaraunt, it is clear that Yelp is keeping a lot of the review data for themselves. They returned the overall restaurant `rating`, but they provide only a small bit of text (`snippet_text`) instead of the full reviews and ratings. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40\n",
      "20\n"
     ]
    }
   ],
   "source": [
    "print chapel_hill_restaurants['total']\n",
    "print len(chapel_hill_restaurants['businesses'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, they cap the total number of business the search will return at 40 and only provide 20 results for each API call.\n",
    "\n",
    "Even with these restrictions, it still might be useful for social science research. As before, you would likely want to define a function in order to make repeated calls to the API. In this, the easier solution might be to create two functions. One that gets a single page and another which retrieves both pages for a single geographical area by calling the first function twice. While it would be possible to do this with zero or one new functions, creating two functions allows for better control over finding and debugging errors since you can test each function independently. Creating lots of small functions generally the code more readable, especially in case like this where you are looping over pages within restaurants within geographic areas. In general, I think the principle of a workflow consisting of small functions, as is commonly found in Python code, is something that social scientists should adopt even when they aren't writing Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_yelp_page(location, offset):\n",
    "    '''\n",
    "    Retrieve one page of results from the Yelp API\n",
    "    Returns a JSON file\n",
    "    '''\n",
    "    # from https://github.com/Yelp/yelp-api/tree/master/v2/python\n",
    "    consumer_key    = 'qDBPo9c_szHVrZwxzo-zDw'\n",
    "    consumer_secret = '4we8Jz9rq5J3j15Z5yCUqmgDJjM'\n",
    "    token           = 'jeRrhRey_k-emvC_VFLGrlVHrkR4P3UF'\n",
    "    token_secret    = 'n-7xHNCxxedmAMYZPQtnh1hd7lI'\n",
    "    \n",
    "    consumer = oauth2.Consumer(consumer_key, consumer_secret)\n",
    "    \n",
    "    url = 'http://api.yelp.com/v2/search?category_filter=restaurants&location=%s&sort=1&offset=%s' % (location, offset)\n",
    "    \n",
    "    oauth_request = oauth2.Request('GET', url, {})\n",
    "    oauth_request.update({'oauth_nonce': oauth2.generate_nonce(),\n",
    "                          'oauth_timestamp': oauth2.generate_timestamp(),\n",
    "                          'oauth_token': token,\n",
    "                          'oauth_consumer_key': consumer_key})\n",
    "    \n",
    "    token = oauth2.Token(token, token_secret)\n",
    "    \n",
    "    oauth_request.sign_request(oauth2.SignatureMethod_HMAC_SHA1(), consumer, token)\n",
    "    \n",
    "    signed_url = oauth_request.to_url()\n",
    "    resp = requests.get(url=signed_url)\n",
    "    return resp.json()\n",
    "\n",
    "def get_yelp_results(location):\n",
    "    '''\n",
    "    Retrive both pages of results from the Yelp API\n",
    "    Returns a dataframe\n",
    "    '''\n",
    "    df = pd.DataFrame()\n",
    "    for offset in [1,21]:\n",
    "        results = get_yelp_page(location, offset)\n",
    "        new_df = pd.DataFrame(results['businesses'])\n",
    "        df = df.append(new_df , ignore_index=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39\n"
     ]
    }
   ],
   "source": [
    "ch_df = get_yelp_results('Chapel Hill, NC')\n",
    "\n",
    "print len(ch_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([u'categories', u'display_phone', u'id', u'image_url', u'is_claimed',\n",
       "       u'is_closed', u'location', u'menu_date_updated', u'menu_provider',\n",
       "       u'mobile_url', u'name', u'phone', u'rating', u'rating_img_url',\n",
       "       u'rating_img_url_large', u'rating_img_url_small', u'review_count',\n",
       "       u'snippet_image_url', u'snippet_text', u'url'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ch_df.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>categories</th>\n",
       "      <th>review_count</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Khushi Salads &amp; Wraps</td>\n",
       "      <td>[[Salad, salad], [Indian, indpak], [Vegan, veg...</td>\n",
       "      <td>16</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>Tarantini</td>\n",
       "      <td>[[Italian, italian]]</td>\n",
       "      <td>67</td>\n",
       "      <td>4.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Imbibe</td>\n",
       "      <td>[[American (New), newamerican], [Wine Bars, wi...</td>\n",
       "      <td>15</td>\n",
       "      <td>4.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Al's Burger Shack</td>\n",
       "      <td>[[Burgers, burgers]]</td>\n",
       "      <td>257</td>\n",
       "      <td>4.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Merritt's Store &amp; Grill</td>\n",
       "      <td>[[Sandwiches, sandwiches], [Convenience Stores...</td>\n",
       "      <td>219</td>\n",
       "      <td>4.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Sunrise Biscuit Kitchen</td>\n",
       "      <td>[[Breakfast &amp; Brunch, breakfast_brunch]]</td>\n",
       "      <td>445</td>\n",
       "      <td>4.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Mediterranean Deli</td>\n",
       "      <td>[[Greek, greek], [Mediterranean, mediterranean...</td>\n",
       "      <td>549</td>\n",
       "      <td>4.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Great Harvest Bread Co</td>\n",
       "      <td>[[Sandwiches, sandwiches], [Bakeries, bakeries]]</td>\n",
       "      <td>30</td>\n",
       "      <td>4.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Elements</td>\n",
       "      <td>[[Asian Fusion, asianfusion], [American (New),...</td>\n",
       "      <td>97</td>\n",
       "      <td>4.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Olio &amp; Aceto Cafe</td>\n",
       "      <td>[[Cafes, cafes]]</td>\n",
       "      <td>30</td>\n",
       "      <td>4.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>Rasa Malaysia</td>\n",
       "      <td>[[Malaysian, malaysian]]</td>\n",
       "      <td>29</td>\n",
       "      <td>4.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>The Root Cellar Cafe and Catering</td>\n",
       "      <td>[[Breakfast &amp; Brunch, breakfast_brunch], [Cafe...</td>\n",
       "      <td>77</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Tobacco Road Sports Cafe</td>\n",
       "      <td>[[American (New), newamerican], [Sports Bars, ...</td>\n",
       "      <td>76</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Bin 54 Steak and Cellar</td>\n",
       "      <td>[[Steakhouses, steak]]</td>\n",
       "      <td>121</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Jujube</td>\n",
       "      <td>[[Asian Fusion, asianfusion]]</td>\n",
       "      <td>148</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Il Palio</td>\n",
       "      <td>[[Italian, italian]]</td>\n",
       "      <td>99</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>Sage Café</td>\n",
       "      <td>[[Vegetarian, vegetarian], [Vegan, vegan], [Gl...</td>\n",
       "      <td>110</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>The Pig</td>\n",
       "      <td>[[Barbeque, bbq]]</td>\n",
       "      <td>268</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>Bangkok 54</td>\n",
       "      <td>[[Thai, thai]]</td>\n",
       "      <td>139</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Kitchen</td>\n",
       "      <td>[[French, french]]</td>\n",
       "      <td>155</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Buns</td>\n",
       "      <td>[[Burgers, burgers], [Hot Dogs, hotdog]]</td>\n",
       "      <td>280</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Lucha Tigre</td>\n",
       "      <td>[[Asian Fusion, asianfusion], [Mexican, mexican]]</td>\n",
       "      <td>210</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Toppers Pizza</td>\n",
       "      <td>[[Chicken Wings, chicken_wings], [Pizza, pizza]]</td>\n",
       "      <td>30</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>R &amp; R Grill</td>\n",
       "      <td>[[American (Traditional), tradamerican], [Burg...</td>\n",
       "      <td>84</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Roots</td>\n",
       "      <td>[[Breakfast &amp; Brunch, breakfast_brunch], [New ...</td>\n",
       "      <td>28</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ms. Mong</td>\n",
       "      <td>[[Mongolian, mongolian], [Asian Fusion, asianf...</td>\n",
       "      <td>101</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Linda's Bar and Grill</td>\n",
       "      <td>[[American (Traditional), tradamerican], [Cock...</td>\n",
       "      <td>111</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>TRU Deli &amp; Wine</td>\n",
       "      <td>[[Delis, delis], [Sandwiches, sandwiches], [Co...</td>\n",
       "      <td>86</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Cholanad</td>\n",
       "      <td>[[Indian, indpak], [Vegan, vegan]]</td>\n",
       "      <td>231</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Sandwhich</td>\n",
       "      <td>[[Sandwiches, sandwiches], [American (New), ne...</td>\n",
       "      <td>237</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Lantern</td>\n",
       "      <td>[[Asian Fusion, asianfusion], [Bars, bars], [V...</td>\n",
       "      <td>290</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Elaine's On Franklin</td>\n",
       "      <td>[[American (New), newamerican]]</td>\n",
       "      <td>123</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Talullas</td>\n",
       "      <td>[[Middle Eastern, mideastern], [Turkish, turki...</td>\n",
       "      <td>162</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Kipos Greek Taverna</td>\n",
       "      <td>[[Greek, greek]]</td>\n",
       "      <td>209</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>City Kitchen</td>\n",
       "      <td>[[American (New), newamerican]]</td>\n",
       "      <td>162</td>\n",
       "      <td>3.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Cosmic Cantina</td>\n",
       "      <td>[[Mexican, mexican], [Vegan, vegan], [Tex-Mex,...</td>\n",
       "      <td>91</td>\n",
       "      <td>3.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Lime and Basil</td>\n",
       "      <td>[[Vietnamese, vietnamese]]</td>\n",
       "      <td>186</td>\n",
       "      <td>3.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Mixed Casual Korean Bistro</td>\n",
       "      <td>[[Korean, korean], [Asian Fusion, asianfusion]]</td>\n",
       "      <td>162</td>\n",
       "      <td>3.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Crook's Corner</td>\n",
       "      <td>[[Southern, southern], [Breakfast &amp; Brunch, br...</td>\n",
       "      <td>181</td>\n",
       "      <td>3.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 name  \\\n",
       "5               Khushi Salads & Wraps   \n",
       "38                          Tarantini   \n",
       "8                              Imbibe   \n",
       "18                  Al's Burger Shack   \n",
       "23            Merritt's Store & Grill   \n",
       "24            Sunrise Biscuit Kitchen   \n",
       "12                 Mediterranean Deli   \n",
       "32             Great Harvest Bread Co   \n",
       "31                           Elements   \n",
       "33                  Olio & Aceto Cafe   \n",
       "34                      Rasa Malaysia   \n",
       "21  The Root Cellar Cafe and Catering   \n",
       "30           Tobacco Road Sports Cafe   \n",
       "28            Bin 54 Steak and Cellar   \n",
       "27                             Jujube   \n",
       "26                           Il Palio   \n",
       "35                          Sage Café   \n",
       "36                            The Pig   \n",
       "37                         Bangkok 54   \n",
       "22                            Kitchen   \n",
       "0                                Buns   \n",
       "20                        Lucha Tigre   \n",
       "10                      Toppers Pizza   \n",
       "2                         R & R Grill   \n",
       "3                               Roots   \n",
       "4                            Ms. Mong   \n",
       "7               Linda's Bar and Grill   \n",
       "9                     TRU Deli & Wine   \n",
       "11                           Cholanad   \n",
       "13                          Sandwhich   \n",
       "14                            Lantern   \n",
       "15               Elaine's On Franklin   \n",
       "16                           Talullas   \n",
       "17                Kipos Greek Taverna   \n",
       "29                       City Kitchen   \n",
       "1                      Cosmic Cantina   \n",
       "6                      Lime and Basil   \n",
       "25         Mixed Casual Korean Bistro   \n",
       "19                     Crook's Corner   \n",
       "\n",
       "                                           categories  review_count  rating  \n",
       "5   [[Salad, salad], [Indian, indpak], [Vegan, veg...            16     5.0  \n",
       "38                               [[Italian, italian]]            67     4.5  \n",
       "8   [[American (New), newamerican], [Wine Bars, wi...            15     4.5  \n",
       "18                               [[Burgers, burgers]]           257     4.5  \n",
       "23  [[Sandwiches, sandwiches], [Convenience Stores...           219     4.5  \n",
       "24           [[Breakfast & Brunch, breakfast_brunch]]           445     4.5  \n",
       "12  [[Greek, greek], [Mediterranean, mediterranean...           549     4.5  \n",
       "32   [[Sandwiches, sandwiches], [Bakeries, bakeries]]            30     4.5  \n",
       "31  [[Asian Fusion, asianfusion], [American (New),...            97     4.5  \n",
       "33                                   [[Cafes, cafes]]            30     4.5  \n",
       "34                           [[Malaysian, malaysian]]            29     4.5  \n",
       "21  [[Breakfast & Brunch, breakfast_brunch], [Cafe...            77     4.0  \n",
       "30  [[American (New), newamerican], [Sports Bars, ...            76     4.0  \n",
       "28                             [[Steakhouses, steak]]           121     4.0  \n",
       "27                      [[Asian Fusion, asianfusion]]           148     4.0  \n",
       "26                               [[Italian, italian]]            99     4.0  \n",
       "35  [[Vegetarian, vegetarian], [Vegan, vegan], [Gl...           110     4.0  \n",
       "36                                  [[Barbeque, bbq]]           268     4.0  \n",
       "37                                     [[Thai, thai]]           139     4.0  \n",
       "22                                 [[French, french]]           155     4.0  \n",
       "0            [[Burgers, burgers], [Hot Dogs, hotdog]]           280     4.0  \n",
       "20  [[Asian Fusion, asianfusion], [Mexican, mexican]]           210     4.0  \n",
       "10   [[Chicken Wings, chicken_wings], [Pizza, pizza]]            30     4.0  \n",
       "2   [[American (Traditional), tradamerican], [Burg...            84     4.0  \n",
       "3   [[Breakfast & Brunch, breakfast_brunch], [New ...            28     4.0  \n",
       "4   [[Mongolian, mongolian], [Asian Fusion, asianf...           101     4.0  \n",
       "7   [[American (Traditional), tradamerican], [Cock...           111     4.0  \n",
       "9   [[Delis, delis], [Sandwiches, sandwiches], [Co...            86     4.0  \n",
       "11                 [[Indian, indpak], [Vegan, vegan]]           231     4.0  \n",
       "13  [[Sandwiches, sandwiches], [American (New), ne...           237     4.0  \n",
       "14  [[Asian Fusion, asianfusion], [Bars, bars], [V...           290     4.0  \n",
       "15                    [[American (New), newamerican]]           123     4.0  \n",
       "16  [[Middle Eastern, mideastern], [Turkish, turki...           162     4.0  \n",
       "17                                   [[Greek, greek]]           209     4.0  \n",
       "29                    [[American (New), newamerican]]           162     3.5  \n",
       "1   [[Mexican, mexican], [Vegan, vegan], [Tex-Mex,...            91     3.5  \n",
       "6                          [[Vietnamese, vietnamese]]           186     3.5  \n",
       "25    [[Korean, korean], [Asian Fusion, asianfusion]]           162     3.5  \n",
       "19  [[Southern, southern], [Breakfast & Brunch, br...           181     3.5  "
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ch_df[['name','categories','review_count','rating']].sort_values(by='rating', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function expects that the first thing you input will be a location. Taking advantage of both `oath2`'s ability to clean up the text so that it is functional when put in a URL (e.g., escape spaces) and Yelp's savvy ability to parse locations, the value for location can be fairly wide (e.g., \"Chapel Hill\" or \"90210\"). You can also add a category of business to search for from the [list](http://www.yelp.com/developers/documentation/category_list) of acceptable values. If you don't provide a value, `category_filter = 'restaurants'` provides a default value of 'restaurants'. This function returns the JSON formatted results. Note that this doesn't have any mechanism for handling errors, which will need to happen elsewhere."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'get_yelp_businesses' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-58-d452f4cfcade>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mchapel_hill_restaurants\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_yelp_businesses\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Chapel Hill, NC'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m21\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mbusiness\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mchapel_hill_restaurants\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'businesses'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mprint\u001b[0m \u001b[0;34m'%s - %s (%s)'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbusiness\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'rating'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbusiness\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'name'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbusiness\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'review_count'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'get_yelp_businesses' is not defined"
     ]
    }
   ],
   "source": [
    "chapel_hill_restaurants = get_yelp_businesses('Chapel Hill, NC', 21)\n",
    "for business in chapel_hill_restaurants['businesses']:\n",
    "    print '%s - %s (%s)' % (business['rating'], business['name'], business['review_count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "beverly_hills_restaurants = get_yelp_businesses('90210')\n",
    "for business in beverly_hills_restaurants['businesses']:\n",
    "    print '%s - %s (%s)' % (business['rating'], business['name'], business['review_count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'/Users/nealcaren/Documents/Collecting-Text'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
